{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f909b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('16P.csv',engine='python', encoding='cp1252')\n",
    "df.drop(\"Response Id\",axis=1, inplace=True)\n",
    "df.replace([\"ESTJ\",\"ENTJ\",\"ESFJ\",\"ENFJ\",\"ISTJ\",\"ISFJ\",\"INTJ\",\"INFJ\",\"ESTP\",\"ESFP\",\"ENTP\",\"ENFP\",\"ISTP\",\"ISFP\",\"INTP\",\"INFP\"],[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],inplace=True)\n",
    "#Normalization\n",
    "df_np= df.to_numpy()\n",
    "df_np_n = df.to_numpy()\n",
    "new=df_np_n[:,:-1]+3\n",
    "df_np_n=new/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List):\n",
    "    counter = 0\n",
    "    num = List[0]\n",
    "     \n",
    "    for i in List:\n",
    "        curr_frequency = List.count(i)\n",
    "        if(curr_frequency> counter):\n",
    "            counter = curr_frequency\n",
    "            num = i\n",
    " \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5d2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_algorithm(k,N):\n",
    "    print(\"All wrong predictions and their indexes and their acculy value\")\n",
    "    if N:\n",
    "        df_np = df_np_n\n",
    "    else:\n",
    "        df = pd.read_csv('16P.csv',engine='python', encoding='cp1252')\n",
    "        df.drop(\"Response Id\",axis=1, inplace=True)\n",
    "        df.replace([\"ESTJ\",\"ENTJ\",\"ESFJ\",\"ENFJ\",\"ISTJ\",\"ISFJ\",\"INTJ\",\"INFJ\",\"ESTP\",\"ESFP\",\"ENTP\",\"ENFP\",\"ISTP\",\"ISFP\",\"INTP\",\"INFP\"],[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],inplace=True)\n",
    "        df_np=df.to_numpy()\n",
    "    train_set, test_set = np.split(df_np, [48001,])\n",
    "#k calculate distances\n",
    "    x2 = np.sum(test_set**2,axis=1) \n",
    "    y2 = np.sum(train_set**2,axis=1)\n",
    "    xy = np.dot(test_set, train_set.T)\n",
    "    x2 = x2.reshape(-1, 1)\n",
    "    dists = np.sqrt(x2 - 2*xy + y2)\n",
    "    argsort = np.argsort(dists) # I found the indexes of its neighbors for each data in the test set in the form of a 2-dimensional numpy array.\n",
    "    \n",
    "# make prediction\n",
    "    indexler=argsort[:,:k]  # i'm taking k neighbors\n",
    "    #  i'm installing the main array here from scratch because the code was corrupted before\n",
    "    df = pd.read_csv('16P.csv',engine='python', encoding='cp1252')\n",
    "    df.drop(\"Response Id\",axis=1, inplace=True)\n",
    "    df.replace([\"ESTJ\",\"ENTJ\",\"ESFJ\",\"ENFJ\",\"ISTJ\",\"ISFJ\",\"INTJ\",\"INFJ\",\"ESTP\",\"ESFP\",\"ENTP\",\"ENFP\",\"ISTP\",\"ISFP\",\"INTP\",\"INFP\"],[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],inplace=True)\n",
    "    df = df.to_numpy()\n",
    "    train,test=np.split(df, [48001,])\n",
    "    matrix_arrayi=np.zeros((16,16))\n",
    "    for y,x in enumerate(indexler,0):\n",
    "        tahmin_listesi=[]\n",
    "        for index in x:\n",
    "            tahmin_listesi.append(df[index,-1])\n",
    "        prediction = most_frequent(tahmin_listesi)\n",
    "        if prediction == test[y,-1] :\n",
    "            matrix_arrayi[prediction][prediction]= matrix_arrayi[prediction][prediction]+1\n",
    "        else:\n",
    "            print((y+48001),\"Prediction:\",prediction,\"Actual:\",test[y,-1])\n",
    "            matrix_arrayi[test[y,-1]][prediction]=matrix_arrayi[test[y,-1]][prediction]+1\n",
    "    accuracy = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "# find TP FP FN FP values for multiple classes\n",
    "    for x in range(16):\n",
    "        TP = matrix_arrayi[x][x]\n",
    "        TN = matrix_arrayi.sum() - np.sum(matrix_arrayi,axis=1)[x] - np.sum(matrix_arrayi,axis=0)[x]\n",
    "        FP = np.sum(matrix_arrayi,axis=0)[x] - matrix_arrayi[x][x]\n",
    "        FN = np.sum(matrix_arrayi,axis=1)[x] - matrix_arrayi[x][x]\n",
    "        xclas_accuracy= (TP + TN )/ matrix_arrayi.sum()\n",
    "        xclas_precision = TP / (TP + FP)\n",
    "        xclas_recall = TP / (TP + FN )\n",
    "        accuracy =accuracy+ xclas_accuracy\n",
    "        precision =precision + xclas_precision\n",
    "        recall =recall + xclas_recall\n",
    "        \n",
    "    accuracy= accuracy / 16\n",
    "    precision = precision / 16\n",
    "    recall = recall / 16\n",
    "    print(\"Accuracy: \",accuracy)\n",
    "    print (\"Precision: \",precision)\n",
    "    print(\"Recall: \",recall)\n",
    "    \n",
    "\n",
    "# If the value of N is T (which means True), normalization is performed. If F is (False), normalization is not performed.    \n",
    "knn_algorithm(3,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee8fb37",
   "metadata": {},
   "source": [
    "Note: I divided the 60k data set into two parts of 48000 and 12000. I did not use cross validation\n",
    "\n",
    "Error Analysis for Classification\n",
    "--------------------------------------------------\n",
    "\n",
    "FEW MISCLASSIFIED SAMPLES: \n",
    "\n",
    "1- index(52296) prediction(8) actual(2) when k = 3   \n",
    "    My program evaluated the data with an index of 52296 as 8 (ESTP), but the correct personality was 2 (ESFJ).\n",
    "    The 3 closest neighbors of this person are: [8, 8, 8] so [ESTP, ESTP,ESTP]\n",
    "    My Opinion For This Situation: All 3 of the nearest neighbors are ESTP, perhaps the person in this index may be acting as a\n",
    "    boundary between the two classes (i.e., at the end of classes ESFJ and ESTP).\n",
    "    \n",
    "2- index(55535), prediction(4), actual(14)\n",
    "    My program evaluated the data with an index of 55535 as 4 (ISTJ), but the correct personality was 14 (INTP).\n",
    "    The 5 closest neighbors of this person are: [4, 4, 14, 14, 12] so [ISTJ,ISTJ,INTP,INTP,ISTP]\n",
    "    My Opinion For This Situation :\n",
    "    The reason for the incorrect prediction of the personality of the person in this index: My algorithm is a field algorithm \n",
    "    that selects which is the closest personality as a prediction if there are two same number of personalities. so in fact, in \n",
    "    this case, the value of k is treated as if it is equal to 1.  This also causes an incorrect estimate.\n",
    "\n",
    "3- index(54156), prediction(12), actual(2) \n",
    "     My program evaluated the data with an index of 55535 as 12 (ISTP), but the correct personality was 4 (ISTJ).\n",
    "     The 3 closest neighbors of this person are: [12, 2, 4] so [ISTP,ESFJ,ISTJ]\n",
    "     My Opinion For This Situation :\n",
    "     If our k parameter were 5 instead of 3, we could get two from any value in the list. That way, we would be more likely to \n",
    "     make the right prediction. Because there is no case of any misclassification error in the outputs where the value of k is\n",
    "     5, and all 5 values are different from each other in the prediction list.\n",
    "\n",
    "4- index(50632), prediction(13), actual(3) \n",
    "    My program evaluated the data with an index of 50632 as 13 (ISFP), but the correct personality was 3 (ENFJ).\n",
    "    The 9 closest neighbors of this person are: [13, 13, 13, 13, 13, 13, 13, 13, 13] so [ISFP,...,ISFP]\n",
    "    My Opinion For This Situation :\n",
    "    This may be due to the reason I gave in the first misclassification example, or it may be because I did not use the k-fold \n",
    "    algorithm.\n",
    "\n",
    "------------------------------------------------\n",
    "\n",
    "k=1\n",
    "accuracy:  0.9361351891981997\n",
    "precision: 0.9781342923350658\n",
    "recall:    0.9781238169299069\n",
    "\n",
    "k=3\n",
    "accuracy:  0.9367602933822302\n",
    "precision: 0.9881567079070425\n",
    "recall:    0.9881318663282883\n",
    "\n",
    "k=5\n",
    "accuracy:  0.9368019669944991\n",
    "precision: 0.9888218597978752\n",
    "recall:    0.9888124342592615\n",
    "\n",
    "k=7\n",
    "accuracy:  0.9368175945990997\n",
    "precision: 0.9890748310420907\n",
    "recall:    0.989064728846451\n",
    "\n",
    "k=9\n",
    "accuracy 0.9368175945990997\n",
    "precision 0.9890799935341432\n",
    "recall 0.9890669396550056\n",
    "\n",
    "k=21\n",
    "accuracy:  0.9368332222037006\n",
    "precision: 0.9893214904642028\n",
    "recall:    0.9893158360146274\n",
    "\n",
    "Performance Analysis For k :\n",
    "    As the value of k increased, I observed increases in all my score values (accuracy,precision,recall) close to 0.001 or \n",
    "    0,0001. While the value of k is 3, the total misclassification error is 130-odd, and while k = 5, the total \n",
    "    misclassification error is 130-odd. I was expecting the number of errors to decrease as the k value increases, the reason \n",
    "    why it does not decrease may be because I have not written a k-fold algorithm in my Knn algorithm.\n",
    "\n",
    "Performance Analysis In My KNN Algorithm: \n",
    "    The part that used the computer RAM the most was the code part of distance calculation. While calculating the distance, I\n",
    "    found that the 16GB RAM is used up to 98%. But when I ran the same code on a computer with 8 RAM, I found that 98% of the \n",
    "    RAM was used.  I was surprised not to get it when I was expecting to get a memory error on an 8 RAM computer. I'm \n",
    "    stillinvestigating why.\n",
    "\n",
    "Performance Analysis for K-Fold : \n",
    "    Although I couldn't use the K-fold algorithm in my code, I did a lot of research about . Therefore, if I used this \n",
    "    algorithm, I can predict that my score values would increase by 0.0001.\n",
    "\n",
    "Accuracy, Precision and Recall : \n",
    "    As I shared above, scores are very ideal for a general Knn algorithm.\n",
    "    \n",
    "Effect of Normalization : \n",
    "    k=3   with Normalization\n",
    "    accuracy:  0.9367602933822302\n",
    "    precision: 0.9881567079070425\n",
    "    recall:    0.9881318663282883\n",
    "    \n",
    "    k=3   without Normalization :\n",
    "    accuracy:  0.9367498749791632\n",
    "    precision: 0.9879904070574\n",
    "    recall:    0.9879649625646864\n",
    "    I have observed Decays around 0.005 between the data I normalized and did not.    \n",
    "    I observed a decrease of around 0.005 between the data that I normalized and the data that I did not.\n",
    "\n",
    " Note: After implanting the normalization code into my knn algorithm function, I observed an increase in all my scores     \n",
    "     k=3 with Normalization ( implamented in Knn_algorithm function )\n",
    "     accuracy:  0.9369634522420405\n",
    "     precision: 0.9913765948308326\n",
    "     recall:    0.9914025958204421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fa617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
